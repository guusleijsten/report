

%\section{Design Method}\label{sec:design_method}
This chapter focusses on each of the code generation phases and what our contributions are to this compiler. This chapter describes how a compiler without support for explicit datapaths (which is described in Section \ref{sec:basic_compiler_design}) is maintained and extended to a compiler with support for explicit datapaths. 

%Discuss maintaining, process of porting from llvm3.8 to LLVM4.0, and discuss new version LLVM5.0. Mention that this upgrade to 4.0 also drastically improved vectorization because the community develops this constantly. (maybe add example of CNN)

%TODO: move vectorization to here? Noooo.

\section{Back-end Code Generation}\label{sec:code_generation}
This section briefly discusses each of the code generation stages, which includes custom passes and standard passes supplied by the LLVM framework. Before generating code, it uses LLVM's front-end to translate a high-level language to an intermediate representation, called LLVM-IR which can be further optimized and used as input for our back-end. The code generation passes in the back-end specific compiler can be categorized in three major categories.
%include an image with the pipeline having all these components.

%TODO: refere to problem where instr have no common operand, so it seems unrelated. 

%TODO: add text that explains the color, instruction selection is copied mostly from another architectures, and was already updated according to our architecture. / Hazard recognizer has partially been copied from other architectures, but was not suitable and has, therefore, been modified and tested. / Delay slot is a custom pass that was already implemented, but we have modified it to be more efficient. / Packetizer pass is as delivered, with a minor modification with hardly any efford.  / Bypass Regs is a custom pass that we developed during the duration of our work.

Firstly, the top row in Figure \ref{fig:simd_backend} shows passes that work on creating a schedule. The second row illustrates a sequence of compilation passes that do back-end specific transformations. Here special features that our architecture has are taken care of. At last, with an LLVM internal representation of back-end specific code for the target architecture, it emits assembly or ELF object code (depicted in the rightmost part of the image) that the processor can understand.

\begin{figure}[H]
\centering
\hspace*{-.12in}
\includegraphics[width=\textwidth]{figures/code_generation}
%TODO: change orange to yellow and yellow to orange in picture. 
\caption{Overview of the phases that the back-end is comprised of.}
\label{fig:simd_backend}
\end{figure}

Note that the passes are marked from light to dark, where a black background means that the pass was created during this project and are the main contributions of this work. Dark grey indicates that a pass is extended or improved, light grey indicates the pass has hardly changed and a white background means that the pass is supplied by LLVM.

%A list of all components that follow in the following section.
\begin{itemize}
	\item \textbf{Instruction Selection:} Uses a DAG, called \texttt{SelectionDAGs}, an abstraction for code representation suitable for phases ranging from instruction selection, legalization to lowering. Instruction selection is implemented in  \texttt{SimdISelDAGToDAG}, which is derived from \texttt{SelectionDAGISel} and consists of a bunch of transformations to transform specific instructions into instruction that are supported by our architecture. For example, transforming operations on immediate values with a value higher than one byte is partially implemented here. Lowering nodes of a DAG is done in \texttt{SimdISelLowering}, which is derived from \texttt{TargetLowering}. There the SIMD intrinsics and ISD instructions are lowered to \emph{Machine Instructions} (MI) and sequences of MIs.
%\item \textbf{Scheduling:} Transforms a directed acyclic graph into an ordered list of instructions.
%\item \textbf{Register allocation:} Assigns physical registers to virtual registers of a list of instructions in SSA form.
	\item \textbf{Instruction Scheduler:} MIScheduler supplied by LLVM\\
MIScheduler is an instruction scheduler which supports VLIW scheduling. Considering there are two issue slots in this architecture, a VLIW scheduler would meet our requirements very well. One and two schedulers are defined for four-stage pipeline and five-stage pipeline respectively. The four-stage pipeline scheduler is the default one while the five-stage pipeline has the default one, and an additional post-register allocation scheduler.
	\item \textbf{Register Allocation:} Greedy Allocator supplied by LLVM\\
	The Greedy allocator is a default allocator in LLVM. Since there is no specific requirement to register allocation, for now, the default allocator is suitable. Apart from the default register allocator, there are more register allocators to choose from and, it is possible to implement a custom register allocator.
	\item \textbf{Post-Register Allocation Scheduler:} A second scheduling pass which is only needed when generating code for a five-stage pipeline configuration, and is disabled otherwise. The post-RA scheduler uses a hazard recognizer which decides whether to prefer certain instruction over other instructions to detect and resolve RaW hazards.
	\item \textbf{Hazard Recognizer:} Consecutive instructions may have hazards with the five-stage pipeline configured. That is when an instruction uses an operand defined in the previous cycle but has a latency of two cycles. For this thesis, we have implemented LLVM's \texttt{ScoreboardHazardRecognizer} to detect and resolve these hazards.
	\item \textbf{Branch Optimizer:} An inefficiency in the generated code was observed where two jump operations control the work flow, but one jump is not necessary when it jumps to the successor that follows immediately. 
	%TODO: check implements / extends / derived
	\item \textbf{Delay Slot Filler:} For this architecture, jump and branch instructions modify the program counter (PC) during the instruction decode stage. At that point, the instruction that follows the jump has already been fetched. The slot that follows a jump or branch instruction is called a delay slot. This pass aims to utilize delay slots by filling them with useful instructions.% that come after a jump or conditional branch instruction, i.e. instructions that modify the program counter.%VERIFY: `or e.g.?
	\item \textbf{Immediate Extension:} In principle, immediate operations have a one-byte immediate operand. However, sometimes you may want to use a larger immediate. This pass allows us to use a larger immediate operand using \texttt{zimm} and \texttt{simm} instruction. These instructions have an 18-bit operand that allows for larger constants to be used.% will be a prefix of the original 8 bits  or using a shift to put a value in a register (up to 32 bits).
	\item \textbf{Packetizer:} This pass creates VLIW bundles that consist of a scalar and a vector instruction, by consulting whether an issue slot is available on which the operation can execute. This pass implements VLIW packetizer supplied by the LLVM framework.
	\item \textbf{Explicit Bypassing:} This pass exploits explicit datapaths in the generated code by using the bypass network. This is developed as a post-processing pass, but can be replaced by any of the approaches discussed in Chapter \ref{sec:expl_bp_impl}.
	\item \textbf{Instruction Printer:} MC is a sub-project of LLVM, which uses \texttt{MCInst} to represent an instruction which is different from the code generators notion of a \texttt{MachineInstr}. MC is used during the last code generation stage when printing the instructions to a given output format. Printing for different output formats is further divided in binary format and SIMD assembly language in XAS-format. 
\end{itemize}

\begin{table}[t!]
\caption{Relations between architecture design features and code generation phases.}
\begin{center}
\begin{tabular}{@{}l l l@{}}
\toprule
\textbf{Feature} & \textbf{Code Gen. Pass} & \textbf{Explanation}\\ \hline
Hardware Pipeline 	& Delay Slot Filler 		& This pass utilizes the delay slots (which is\\
				&					& a product lockstep pipelining).\\
			 	& Hazard Recognizer 	& This pass is necessary for a five-stage \\
				&					& pipeline configuration where not all\\
				&					& instructions have a single cycle latency.\\
Bit-width 			& Immediate Extension 	& Lowering immediate operands is done\\
				&				    	& differently for different data bit-width. \\
ISA extension		& Instruction Selection 	& New instructions need to be described in\\
				& \& Instruction Lowering	& the back-end. \\
Explicit datapaths 	& Explicit Bypassing		& This optional pass is developed to exploit\\
				&					& explicit datapaths. It can be enabled\\
				&					& using compiler flag \texttt{-explicit}. \\
\bottomrule
\end{tabular}
\end{center}
\label{table:rel_feature_pass}
\end{table}%

The following sections give a more elaborate discussion on each custom pass and describe their relation to each other. However, before that have a look at Table \ref{table:rel_feature_pass} which gives an overview of the passes responsible for each of the features from Chapter \ref{sec:problem_statement}. 

%TODO: validate and update
\section{Custom Passes}
This section describes the custom passes in more detail. Dependencies and relations to other passes are described. Furthermore, it will discuss the entire toolchain, which includes an assembler, a linker, and a simulator. 

\subsection{Hazard Recognizer}\label{sec:hazard_recogn}
When generating code for a four-stage pipeline configuration, all instructions have a latency of one cycle. In that case, hazard recognition is not necessary.
In order to support code generation for a five-stage pipeline, a hazard recognizer has been developed. The hazard recognizer is used by the post-RA scheduler to determine whether two consecutive instructions can be scheduled after each other. 

\lstset{style=customasm}
\begin{lstlisting}
addi r13, r0, 14
addi r12, r0, 10
mul  r2,  r5, r13  # latency=2
mul  <@\hspace{1px}\textcolor{red!70!black}{r3}\hspace{1px}@>,  r6, r12  # latency=2
add  r2,  r2, <@\hspace{1px}\textcolor{red!70!black}{r3}\hspace{1px}@>   # RaW dependency
\end{lstlisting}

It detects hazards by considering whether an operation has a RaW dependency with instruction that came prior to it. A true hazard is when the instruction that came prior to it has a RaW dependency and a latency of more than one cycle. The \emph{post-register allocation} (Post-RA) scheduler does a linear scan through the list of operation and queries the hazard recognizer whether an instruction can be scheduled. When it detects a hazard, it will consider other instructions that are ready to be scheduled, and if there are none available without a hazard, it will insert a no-op and the processor will be stalled for a cycle.

%It recognizes hazards by looking at whether the current instruction to be scheduled uses a register that is defined by the instruction issued in the previous cycle, which also has a latency of more than one cycle. If this is the case, we have a hazard and the instruction under consideration can not be scheduled in the current cycle. At this point we will consider other instructions that are ready to be scheduled and if there are none available without hazards, we will insert a no-op.

\subsection{Branch Optimizer}
%WORKING RIGHT HERE. ADD EXAMPLE CASES FROM BRANCH OPT.
There were many double branch instructions in loop structures. Preliminary benchmarks already showed that a double branch instruction does not always help. For example, consider the assembly code fragments in Listing \ref{lst:br_opt_1} and Listing \ref{lst:br_opt_2}. Note that at this point, the delay slots that always follow after a jump or branch instruction are still absent because the delay slot filler pass has not run yet.

\captionof{lstlisting}{Fragment of assembly code to illustrate the first and second cases that are considered by the branch optimization pass.}\label{lst:br_opt_1}
\begin{center}
\hspace{2px}\begin{minipage}{.475\textwidth}
\begin{lstlisting}[frame=tlrb]
$BB1:
    addi   r1, r1, 1
    sfltsi r1, 64
    <@\hspace{1px}\textcolor{red!70!black}{bf}\hspace{1px}@>     <@\hspace{1px}\textcolor{red!70!black}{\$BB1}\hspace{1px}@>
    <@\hspace{1px}\textcolor{red!70!black}{j}\hspace{1px}@>      <@\hspace{1px}\textcolor{red!70!black}{\$BB2}\hspace{1px}@>
$BB2:      <@ $\hdots$@>
\end{lstlisting}
\end{minipage}\hfill
\begin{minipage}{.475\textwidth}
\begin{lstlisting}[frame=tlrb]
$BB1:
    addi   r1, r1, 1
    sfltsi r1, 64
    <@\hspace{1px}\textcolor{red!70!black}{bf}\hspace{1px}@>     <@\hspace{1px}\textcolor{red!70!black}{\$BB1}\hspace{1px}@>
$BB2:      <@ $\hdots$@>
           <@ $\hdots$@>
\end{lstlisting}
%\vspace{1.9em}
\end{minipage}
\end{center}

\begin{enumerate}
  \item The first case is where a block ends with a jump instruction to the block that immediately follows. The example shows a loop in which a counter is incremented until it reaches sixty-four. As long as the counter is less than that, the flag will be true and the branch is executed. When the counter increases, at some point the condition breaks and the flag becomes false. The program counter then points to the first instruction after the branch instruction, which is a jump instruction. However, the jump goes to the successor that immediately follows. When the jump is removed, the first instruction after the branch instruction is still the successor block that immediately follows. Hence the behaviour without that jump is identical and the superfluous jump can be removed.
  \item The second case has a branch instruction to the block that immediately follows and a jump to somewhere else. The first step then is to reverse the branch condition. This can be achieved by changing a \texttt{bf} (branch flag) instruction into a \texttt{bnf} (branch not flag) and vice versa. After the branch is reversed, the situation becomes a jump instruction to the block that immediately follows and a branch instruction to somewhere else. 
\end{enumerate}

\captionof{lstlisting}{Fragment of assembly code to illustrate the third case that is covered by the branch optimization pass.}\label{lst:br_opt_2}
\begin{center}
\hspace{2px}\begin{minipage}{.475\textwidth}
\begin{lstlisting}[frame=tlrb]
$BB2: <@$\hdots$@>
    sf condition  # set-flag
    <@\hspace{1px}\textcolor{red!70!black}{bf}\hspace{1px}@> <@\hspace{1px}\textcolor{red!70!black}{\$BB3}\hspace{1px}@>
    <@\hspace{1px}\textcolor{red!70!black}{j}\hspace{1px}@>  <@\hspace{1px}\textcolor{red!70!black}{\$BB2}\hspace{1px}@>
$BB3: <@$\hdots$@>
\end{lstlisting}
\end{minipage}\hfill
\begin{minipage}{.475\textwidth}
\begin{lstlisting}[frame=tlrb]
$BB2: <@$\hdots$@>
    sf  condition  # set-flag
    <@\hspace{1px}\textcolor{red!70!black}{bnf}\hspace{1px}@> <@\hspace{1px}\textcolor{red!70!black}{\$BB2}\hspace{1px}@>
$BB3: <@$\hdots$@>
     <@ $\hdots$@>
\end{lstlisting}
%\vspace{1.9em}
\end{minipage}
\end{center}

When a (double) jump intruction(s) jumps to a successor that immediately follows, the branch optimizer can always remove one jump with these cases.



%END BRANCH OPT DESCRIPTION

\subsection{Delay Slot Filler}
During the execution of a conditional branch or jump instruction the \emph{program counter} (PC) is modified while the instruction is in the \emph{instruction decode} (ID) stage. A side product of lockstep pipelining which is introduced in Chapter \ref{sec:datapaths}, is that when a jump instruction is being decoded in the ID stage, the next instruction with $PC = PC+4$ has already been fetched from IMEM before the jump is executed. Therefore, the instruction that is followed by the jump instruction is executed presumptuously, which is referred to as a delay slot.

%This slot will be executed before the instruction that the PC points at after modifying the PC and is called delay slot. %TODO: this paragraph can be shorter 

In order assure correct behaviour this pass intentionally 
%or initially (stood there in the first place)
places a no-op after each jump or branch instruction. However, sometimes it can do better. Namely, it can use an instruction from before the jump instead of a no-op. This pass performs a backwards search to look at the two instructions before the jump, and the instruction that comes after the jump, which are referred to as $prev_1$, $prev_2$, and $next$ respectively. When the backwards search does not fill the delay slot it intentionally insert a no-op, and if there is a vector operation that comes after the delay slot, it also inserts a vector-nop, so that the packetizer does not bundle it with the delay slot later on.

%\captionof{lstlisting}{Fragment of assembly code to illustrate behaviour of the delay slot filler.}\label{lst:delayslot1}
%\begin{center}
%\hspace{2px}\begin{minipage}{.475\textwidth}
%\begin{lstlisting}[frame=tlrb]
%BB0_1:
%    sfne r1, 7
%    add r3, r3, r1
%    bf BB0_1
%    nop
%\end{lstlisting}
%\end{minipage}\hfill
%\begin{minipage}{.475\textwidth}
%\begin{lstlisting}[frame=tlrb]
%BB0_1:
%    sfne r1, 7
%    bf BB0_1
%    add r3, r3, r1
%   <@ @>
%\end{lstlisting}
%%\vspace{1.9em}
%\end{minipage}
%\end{center}

%Possibly scrap this case distinction 
%Add picture from scratch paper showing these cases

\captionof{lstlisting}{Fragment of assembly code to illustrate behaviour of the delay slot filler for case one.}\label{lst:delayslot1}
\begin{center}
\hspace{2px}\begin{minipage}{.475\textwidth}
\begin{lstlisting}[frame=tlrb]
$BB1:
    v.add r3, r6, r14
    v.add r4, r5, r12
    j $BB1
    nop
\end{lstlisting}
\end{minipage}\hfill
\begin{minipage}{.475\textwidth}
\begin{lstlisting}[frame=tlrb]
$BB1:
    v.add r3, r6, r14
    j $BB1
    nop
    v.add r4, r5, r12
\end{lstlisting}
%\vspace{1.9em}
\end{minipage}
\end{center}

\begin{enumerate}
\item The first case is when the previous two instructions are both vector instructions. In that case, the vector instruction prior to the jump instruction can be moved to after it. Now the vector instruction that remains before the jump ($prev_1$) gets bundled with the jump instruction. If there was originally a scalar instruction after the jump instruction, it could get bundled with the vector instruction that is moved by this pass, later on by the packetizer. Therefore, it inserts a no-op to ensure the delay slot. %the instruction that came after is a scalar instruction, it would be merged with the filled instruction, therefore, in that case we need to add a nop, such that it bundles with the filled vector instruction.
\item When the instruction before the jump ($prev_1$) is a scalar instruction with no dependencies to the jump itself, it can moved to the delay slot. If a bundled instruction was moved, then it is done. Otherwise, if there are two vector operations prior to the jump instruction, it can move one of them to after the jump, thereby, fully utilizing the delay slot.

%\item[3] We also consider the case where $prev_1$ is a vector instruction, and $prev_2$ is not a relational or jump instruction. Since the first case considers both $prev_1$ and $prev_2$ vector instructions, we consider cases where $prev_1$ is a vector and $prev_2$ is scalar operation by this case. %todo: little more in depth explanation of this pass
\end{enumerate}

\captionof{lstlisting}{Fragment of assembly code to illustrate behaviour of the delay slot filler for case two.}\label{lst:delayslot2}
\begin{center}
\hspace{2px}\begin{minipage}{.475\textwidth}
\begin{lstlisting}[frame=tlrb]
$BB1:
    v.add r3, r6, r14
    v.add r4, r5, r12
    add   r3, r3, r1
    j $BB1
    nop
\end{lstlisting}
\end{minipage}\hfill
\begin{minipage}{.475\textwidth}
\begin{lstlisting}[frame=tlrb]
$BB0_1:
    v.add r3, r6, r14
    j $BB1
    add   r3, r3, r1
    v.add r4, r5, r12
   <@ @>
\end{lstlisting}
%\vspace{1.9em}
\end{minipage}
\end{center}

%\captionof{lstlisting}{Fragment of assembly code to illustrate behaviour of the delay slot filler for the third case.}\label{lst:delayslot3}
%\begin{center}
%\hspace{2px}\begin{minipage}{.475\textwidth}
%\begin{lstlisting}[frame=tlrb]
%BB0_1:
%    add r3, r3, r1
%    v.add r4, r5, r12
%    j BB0_1
%    nop
%\end{lstlisting}
%\end{minipage}\hfill
%\begin{minipage}{.475\textwidth}
%\begin{lstlisting}[frame=tlrb]
%BB0_1:
%    j BB0_1
%    add r3, r3, r1
%    v.add r4, r5, r12
%   <@ @>
%\end{lstlisting}
%\vspace{1.9em}
%\end{minipage}
%\end{center}
%todo: give case(s) that is not covered, followed by this line
The two cases covered by this pass are illustrated in Listing \ref{lst:delayslot1} and \ref{lst:delayslot2}. However, many delay slots are still not being utilized. Extending this pass such that more delay slots may be utilized will be added to future work.

%TODO: make case distinction here.
%\begin{enumerate}
%	 \item When the two operation before the jump instruction are both vector instructions,  
%\end{enumerate}
%TODO: make pseudo code of 

%Immediate extension 
\subsection{Immediate Extention}\label{sec:immediate_ext}
Most operations in Appendix \ref{appendix:i_type_instrs} have as last operand, a one byte immediate. However, sometimes one may need larger numbers. Therefore, constants are lowered during instruction selection. The following three cases are given:
\begin{enumerate}
\item When the immediate can be expressed with one byte it is trivial. It does not need to change anything.
\item When the immediate value is larger than one byte, but can be expressed with 26 bits, it adds a \texttt{zimm} or \texttt{simm} operation in front of it. These operations have a 18 bit immediate that represent the upper 18 bits for the operation that follows.

\begin{lstlisting}
simm 3          # 3 << 8 = 768
addi r3, r5, 12 # r3 = r5 + 768 + 12
\end{lstlisting}
\item If the immediate requires more than 26 bits, it requires a couple of instructions to be added in order to put the immediate value in a register. Firstly, the upper 6 bits go to a register and are shifted all the way to the left. Subsequently, the lower 26 bits are added to it using the previous cases.
%TODO: illustrate how the lowering is done.
\begin{lstlisting}
add  r3, r0, 2    # upper 6 bits of the immediate
slli r3, r3, 26   # 2 << 26
zimm 3            # 3 << 8, upper 18 bits
addi r3, r3, 12   # lower 8 bit
\end{lstlisting}
\end{enumerate}

\texttt{ImmExtension} is a class that is derived from \texttt{MachineFunctionPass} and adds a \texttt{zimm} or \texttt{simm} when necessary. Pseudo code for this algorithm can be found in L. Zhenyuan his thesis \cite[Appendix B]{liu_zhenyuan}. \\

The contribution of this work is that \texttt{SimdISelDAGToDAG} (instruction selection) is extended such that a larger range of immediate values is supported. Namely, from 26 to 32 bits. Furthermore, a bug that was found and resolved in the part that inserts \texttt{simm} operations. One may use even operations with more than 32 bits since carry-using operations can be selected. These operations have three operands: The first two are the normal LHS and RHS, and the third is the input carry flag. The operations can then be chained together for adding and subtracting arbitrarily large values.

\subsection{Packetizer}
Using a packetizer transforms a sequential list of mixed scalar and vector operations into VLIW instructions that contain one scalar and one vector instruction. It does this by using \emph{VLIWPacketizerList} from the LLVM framework. 
It searches for packets by going in a top-down approach through the list of operations until the end of the machine function is reached. It aims at filling all operation slots of an instruction, in our case a scalar and a vector operation. If an operation is encountered of a slot which is already full, it ends the packetized instruction and it proceeds to the next packet. 

\captionof{lstlisting}{Illustration of how a list of mixed scalar and vector operations are transformed into 2-issue instructions.}\label{lst:packetizer}
\begin{center}
\hspace{2px}\begin{minipage}{.45\textwidth}
\begin{lstlisting}[frame=tlrb]
v.addi r2,  r0,  a
add    r11, r10, r0
v.addi r3,  r0,  b
v.lw   r2,  r2,  0
v.lw   r3,  r3,  0
v.addi r11, r11, 4
v.mul  r2,  r3,  r2
v.addi r3,  r0,  c
v.sw   r3,  r2,  0
lw     r10, r11, 0
jr r9
addi   r11, r11, 4
\end{lstlisting}
\end{minipage}\hfill
\begin{minipage}{.5\textwidth}
\begin{lstlisting}[frame=tlrb]
add  r11, r10, r0 || v.addi r2,  r0,  a
                  || v.addi r3,  r0,  b
                  || v.lw   r2,  r2,  0
                  || v.lw   r3,  r3,  0
                  || v.addi r11, r11, 4
                  || v.mul  r2,  r3,  r2
                  || v.addi r3,  r0,  c
lw   r10, r11, 0  || v.sw   r3,  r2,  0
jr r9             || v.nop
addi r11, r11, 4  || v.nop

<@\ @>
\end{lstlisting}
%\vspace{1.9em}
\end{minipage}
\end{center}

Listing \ref{lst:packetizer} illustrates the transformation performed by the packetizer. The first two operations get put together in a packet by filling both the scalar and the vector slot. Then the vector operations are put in their own packet because the vector slot is already full. The load word operation is put with the last vector operation, thereby, fully utilizing the packet and the last two scalar operations get their own packet as well. %How do I say this? (of or from or ..) is this the correct way to do it or do you know any better or neater way to do so. 

No contributions have been made to this pass, however, the packetizer is actually used to resolve an issue with the assembler. Without these modifications, each packet may have either one or two operations. However, this is modified to always fill a packet with a no-ops or vector no-ops when it is not full. The assembler translates the operations to binary code and when the VLIW instructions are not full, it will insert only a sub-instruction, which makes it difficult to determine where the next instruction starts. 


%TODO: explain 3.3.

\subsection{Explicit Bypassing}
This pass exploits the bypass network in an explicit manner. Result forwarding and dead result elimination are performed on a generated code. Currently, it does this as a post-processing step, but it may be moved to somewhere else in the compilation chain. In general, the information of which operations reside in the pipeline at a point in time is needed to decide which results can be forwarded. Therefore, the behaviour of the bypass network is modelled at compile-time. The model is then used to decide whether a certain operand of an instruction may be bypassed. When an operand is bypassed, the liveliness information of the register that is bypassed is used to decide whether a certain write access to the RF is still needed. Effectively, if the variable that was bypassed is dead after a use (denoted by a register \emph{kill}) it does not need to be stored anymore, since it will not be used later on. 
 % and going through a list of instructions. %We may then use the model to keep the processor pipeline accurate and use it to allocate 
%While the instruction of a basic block are traversed, we use the model of the bypass network to decide whether we can bypass certain operands.

\captionof{lstlisting}{Fragment of assembly code to illustrate operand forwarding and dead result elimination.}\label{lst:explicit_reg_alloc}
\begin{center}
\hspace{2px}\begin{minipage}{.475\textwidth}
\begin{lstlisting}[frame=tlrb]
lw  r1, r10, 1
lw  r2, r10, 2
mul r1, r1,  r2
sw  r1, r10, 0
\end{lstlisting}
\end{minipage}\hfill
\begin{minipage}{.475\textwidth}
\begin{lstlisting}[frame=tlrb]
lw  r1,  r10, 1
lw  --,  r10, 2
mul --,  r1,  LSU
sw  MUL, r10, 0
\end{lstlisting}
%\vspace{1.9em}
\end{minipage}
\end{center}

The assembly code in the above example starts with loading two values from memory. Subsequently, the values are multiplied and the result is stored back to memory. The second load produces a result that is immediately used. Therefore, it can be forwarded using operand forwarding (which is discussed in Chapter \ref{sec:datapaths}). This is encoded with \texttt{LSU}, because loads are executed by the \emph{load store unit} (LSU). Similarly, the result of the multiplication is immediately used and forwarded. In this case it is encoded with \texttt{MUL}, denoting the functional unit that executes multiplications.

The example in Listing \ref{lst:explicit_reg_alloc} is a self-contained assembly code fragment, so the result of these instructions are not used outside of what you can see. Hence, each result has exactly one use, and is dead after that use. Therefore, the variables that are bypassed will not be read from the RF, because they are forwarded  from the bypass network instead. According to dead result elimination (introduced in Chapter \ref{sec:datapaths}) these obsolete stores can be removed, which is encoded using `\texttt{--}'. When an instruction has this as destination, the write enable is put to low when that instruction is in the writeback stage, and the result of that instruction is not written back to the RF. Reducing communication with the RF leads to an energy efficienty improvement, however, the variable is only available for as long as it resides in the pipeline. 

\section{Source-level Linker}
Figure \ref{fig:linker_A} shows a process to do compilation and simulate the output of the compiler. The resulting assembly code is simulated in order to verify the correctness of a benchmark (C file). The simulation generates a directory with the memory dumps after running the program. It also produces statistics that indicates how often a certain line is executed, and we can deduce from the statistics file how many memory reads and writes, how many register read/writes, and how many bypassed reads and writes there are.

\begin{figure}[H]
\centering
\includegraphics[width=.95\textwidth]{figures/linker_illustration1}
\caption{Workflow of simulation.}
\label{fig:linker_A}
\end{figure}

Doing the compilation and preprocessing steps from the compiler results in unlinked assembly code. Here symbols are not resolved yet, and it does not automatically generate a \emph{\_start} function. The source-level linker resolves the symbols and inserts a \emph{\_start} function in which the stack is initialized. However, the source-level linker works only for a single input file, therefore, all benchmarks are implemented in a single C file. The assembler and the simulator from the legacy toolchain are used and the source-level linker is necessary for the assembler to work. 

\begin{figure}[t]
\centering
\includegraphics[width=.9\textwidth]{figures/linker_illustration2}
\caption{Standard linking process.}
\label{fig:linker_B}
\end{figure}

Figure \ref{fig:linker_B} illustrates the standard linking process when using the LLVM tools to do everything up to simulation. Other students have implemented an assembler and a linker within the LLVM framework. The assembler mainly consists of a parser that parses assembly code, and it uses LLVM-MC to form instructions to print them and the LLVM linker is developed within LLD. However, the new linker and assembler can not be used yet because it is still a work in progress.

%TODO: laat laatste 2 zinnen weg en voeg laatste zin uitgebreid toe



%Implementation of explicit datapaths using bypass registers.

%TODO: decide: comment or uncomment following section expl bp pass
%\subsection{Explicit Bypass Registers}
%This component implements the pass that we discuss in Section \ref{sec:expl_bp_impl} and thereby, implements the main goal of this project. It allocates explicit bypass registers on a machine function and it does that on a per basic block fashion using information of the pipeline state of other basic blocks. It uses \emph{BypassState} which keeps track of the bypass state of each basic block and can be configured to work on a given input state. This way the state of a pipeline can be analysed and remembered by the \emph{SimdExplicitRegister} pass after execution a basic block.

%\subsection{Instruction Printer}

%END COMPONENT DESCRIPTION

%\subsection{Assembler and Disassembler}

