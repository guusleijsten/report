The LLVM project was started in 2000 by Chris Lattner, as a research project at the University of Illinois with the goal of providing a modern, Static Single Assignment (SSA)-based compilation strategy capable of supporting both static and dynamic compilation of arbitrary programming languages. It was first released in 2003 and the project has grown rapidly since then. It has become popular amongst major companies, e.g. Google, Apple, and Sony, for its powerful multi-stage compilation strategy and outstanding extendibility. LLVM is a collection of modular and reusable compiler and toolchain technologies. Generally, LLVM follows a 3-phase design, which is divided between a frontend, a code independent optimizer and a backend, illustrated in Figure \ref{fig:3phase_design}.

\begin{figure}[H]
\centering
\includegraphics[width=.7\textwidth]{figures/3phase_design}
\caption{3-phase design: frontend, optimizer and backend.}
\label{fig:3phase_design}
\end{figure}

%Reorder list: [IR, Lexical analysis, Syntax analysis, ]
\textbf{The frontend} is responsible for translating code of an arbitrary programming language into LLVM's Intermediate Representation (IR) code. The LLVM instruction set represents a virtual architecture that captures the key operations of ordinary processors, but avoids machine specific constraints such as physical registers. Instead, it has an infinite amount of virtual registers in SSA form, which means that each virtual register is assigned only once and each use of a variable is dominated by that variable's definition. This simplifies the data flow optimizations because only a single definition can reach a particular use of a value, and to find that definition is trivial \cite{llvm_strategy}.
%frontend talk, straight from the Dragon Book plz.
%introduce parser and lexical analysis and that it is kept in an AST, which will be translated as a final step to IR.
%Perhaps an example?
%TODO: rewrite first sentence in my own words.
In the first phase of a compiler, the main task of the lexical analyzer is to read the input characters of the source program, group them into luxemes, and produce as output a sequence of tokens. These tokens are used by the parser for syntax analysis, where it is verified that the sequence of tokens can be reconstructed according to the syntax of the input language. The parser reports any syntax errors during this process and should be able to recover from the error in order to continue processing the rest of the program. The parser constructs a parse tree, and the semantic analyzer uses this parse tree to check for consistency with the language definition. Type checking is also done during this stage, and the information is kept in a syntax tree. The result of these phases is an Abstract Syntax Tree (AST) of the program, which can be translated into three-address IR code. %We will discuss LLVM's IR in more detail in Chapter \ref{sec:ir}

\begin{figure}[b!]
\centering
\includegraphics[width=.7\textwidth]{figures/frontend}
\caption{Overview of the components that the frontend compromises.}
\label{fig:frontend}
\end{figure}

\textbf{The optimizer} contains a collection of analysis and semantic-preserving transformations that can be used to optimize IR code. One of the advantages of LLVM is that when you build a new backend for any given processor architecture you immediately have access to all of these optimizations. Below we give some of these optimizations that are explained more detailed in the literature \cite[Chapter~9]{dragon_book}.%TODO: provide chapter in cite.
\begin{itemize}
\item \emph{Constant propagation} computes for each point and each variable in the program, whether that variable has a unique constant value at that point. This can then be used to replace variable references with constant values.
\item \emph{Constant folding} recognizes and evaluates constant expressions at compile time rather than runtime. For example, `$add\ 1+2$' can be replaced by `$3$'. Statements like `$add\ 1+2$' can be introduced by other optimizations, e.g. constant propagation. 
\item \emph{Common sub-expression elimination} recognizes that the same expression appears in more than one place, and that performance can be improved by transforming the code such that the expression appears in one only place.
\item \emph{Copy propagation} replaces each target of a copy statement with that of the copied value. For example, if we have a copy statement, $x = y$. Then the uses of $x$ can be replaced by $y$. Some optimizations require that this optimization is performed afterward to clean up, e.g. common sub-expression elimination requires this pass to run afterward. 
\item \emph{Dead code elimination} removes code that does not affect the program's results. This avoids executing irrelevant operations and reduces the code size of a program.  
\item \emph{Loop invariant code motion} aims at moving code that is independent of the loop iteration out of the loop body. It does this by moving the loop independent statement above the loop, saving it in a temporary variable, and use it in each iteration of the loop. Now the loop independent statement is computed only once instead of every iteration. 
\item \emph{Function inlining} verifies whether inlining functions in its callees gives a performance benefit. If doing this would give a performance benefit, it replaces the call to the function with the function body. This optimization often is useful for small functions because it reduces the overhead that is introduced when a function call is made, e.g. storing frame pointer, storing function parameters and jump to the code to where the function is defined.     
\end{itemize}

\begin{figure}[t]
\centering
\includegraphics[width=.85\textwidth]{figures/code_generation_sequence}
\caption{Code generation sequence, from LLVM code to assembly code.}
\label{fig:code_generation}
\end{figure}

%TODO: apply following namings: Instruction, Machine instruction, scheduled instruction in SSA form, schedule instruction not in SSA form.
\textbf{The backend} translates, according to a processor architecture, IR code to a target specific assembly language. It does this by going through a sequence of code generation stages, illustrated in Figure \ref{fig:code_generation}. The rectangular boxes indicate the data structure that is used by and produced by a given stage, and the name of each stage is denoted in a rectangular box with rounded corners. During this process, first, the IR code is lowered to a Directed Acyclic Graph (DAG) in which each node represents one instruction. However, for some architectures, not all data types and instructions are supported. For this reason, the DAG is legalized to something that is supported by the target architecture. Instruction selection maps each of the nodes into machine nodes, by matching patterns. %After that, the instruction selector maps the pattern of LLVM code into the target machine code and builds a new DAG whose nodes represents the target instructions.
Then we have a DAG consisting of only target specific machine instructions, in SSA form. Having naive machine instructions, the next step is to schedule them. We schedule the machine instructions according to the resource information of the target processor and assign each instruction to a specific cycle. 
%We will discuss scheduling in more detail in Chapter \ref{sec:scheduling}. 
Now the instructions are represented in a list rather than a DAG, but still in SSA form. The Register Allocator (RA) then assigns physical registers to each of the virtual registers, now the list is not in SSA form. 
%We will discuss RA in more detail in Chapter \ref{sec:register_allocation}. 
The post-allocation pass can improve the schedule, taking the physical registers and register pressure, that is known at this point, into account. After that, some epilogue and prologue code might need to be inserted, e.g. saving/restoring the caller/callee registers and reserving/destroying of the function's stack frame. Peephole optimizations are target specific improvements to the schedule that has been constructed. These optimizations deal with very specific optimizations that can only be done at the end of the process. Finally, the assembly printer prints the assembly code.
%backend talk -> huuge



%\subsection{Instruction Scheduling}\label{sec:scheduling}
%After instruction selection, the program is represented in SSA form as a DAG. Each instruction is represented as a $MachineSDNode$ in a $MachineBasicBlock$. After scheduling has been performed, each instructions now is represented as a $MachineInstr$. 



%ach instruction is scheduleDuring instruction schdduling, each $MachineBasicBlock$ is scheduled by the scheduler and transformed into a $MachineInstr$. After this phase, instructions are represented as $MachineInstr$.

%\subsection{Register Allocation}\label{sec:register_allocation}
%Register allocation is executed during the code generation phase and consists of finding a mapping of a program with an unlimited number of virtual registers to a program with a limited number of physical registers.
%Examples, machine instrs on the left in SSA form, on the right with proper registers next to it.
%Introduce at least 

\subsection{Scheduling and Register Allocation}\label{sec:scheduling_and_ra}
From a compiler's perspective, certain instructions address memory locations. \emph{Store} instructions access the memory to put the value of a register into memory at a certain location, addressable by its address. \emph{Loads}, on the other hand load a value from a location in memory to a register. Other instructions calculate a result of an operation and stores the result in a register. Operations that store the result in a register, or load a value from memory into a register, actually define a register. Before scheduling and register allocation, the sequence of instructions contain an unlimited number of virtual registers and the instructions are in a DAG that preserves all control flow and data dependencies. Data dependencies are ordering constraints that influence the order of execution. Typically, there are three kinds of data dependencies \cite{data_dependece}:
\begin{enumerate}
\item There is a Read-after-Write (R/W) dependency, also called a \emph{flow dependence} from operation $a$ to $b$ if $a$ defines a register that is used by $b$.
\item  There is a Write-after-Read (W/R) dependency, also called an \emph{anti-dependency} if for operations $a$ and $b$, we have that $a$ uses a register that is redefined by $b$. 
\item There is a Write-after-Write (W/W) dependency, or \emph{output dependency} from $a$ to $b$ if $a$ defines a register that is redefined by $b$.
\end{enumerate} 
Flow dependencies are also known as \emph{true dependencies} and anti and output dependencies as \emph{false dependencies}, introduced by register allocation. While in SSA form, each variable is defined exactly once, we have only true dependencies in that form. After register allocation where we assign physical registers to each virtual register, we may assign a physical register to multiple virtual registers, see Listing \ref{lst:ra_example}. This process introduces false dependencies and can often be resolved with renaming techniques \cite{renaming}. 

\begin{center}
%TODO: include a data dependency graph for the example below
\captionof{lstlisting}{Redefining physical registers by assigning them to multiple virtual registers.}
\begin{minipage}[t]{.4\textwidth}
\begin{lstlisting}
add <@\textcolor{red}{\%v1}@>, %src1, %src2
add <@\textcolor{red}{\%v2}@>, %src2, %src3
add %v3, <@\textcolor{red}{\%v1}@>, <@\textcolor{red}{\%v2}@>
add %v4, %sp, 4
\end{lstlisting}
\end{minipage}
\begin{minipage}[t]{.4\textwidth}
\begin{lstlisting}
add <@\textcolor{red}{r1}@>, r5, r6
add <@\textcolor{red}{r2}@>, r6, r7
add <@\textcolor{red}{r2}@>, <@\textcolor{red}{r1}@>, <@\textcolor{red}{r2}@>
add <@\textcolor{red}{r1}@>, r8, r9
\end{lstlisting}
\end{minipage}
\label{lst:ra_example}
\end{center}

%TODO: add control flow dependecy explanation, optionally from Code Generation for TTAs p.

Sometimes there are not enough registers available to allocate a physical register to all virtual registers because there are only a limited number of physical registers available. In that case, register \emph{spilling} may be necessary to free one or more registers by storing them in the stack. Then consequently you are required to retrieve them from the stack right before they are needed.

%TODO: insert general RA algorithms and heuristics. Graph colouring, and use this reference \cite{ra}.
There are multiple register allocators available in LLVM, e.g. basic, fast, greedy and Partitioned Boolean Quadratic Problem (PBQP) register allocation. PBQP is a nearly optimal approach that does register allocation in phases, i.e. spilling, register assignment and copy coalescing. After spilling, RA can be done in polynomial time, but copy coalescing is NP-complete \cite{pbqp}. The other three register allocators are linear scan based algorithms that use heuristics and visit live ranges in order, although it is possible to implement a custom register allocator.

%TODO: insert scheduling talk, including algorithms, heuristics and LLVMs available schedulers.

A common problem in compilers is the ordering in which to do scheduling and register allocation. If registers are allocated before scheduling, the resulting code tends to have many storage dependencies that limit code scheduling. On the other hand, if the code is scheduled before register allocation, the schedule created may require so many registers that register spilling is required, which may negate the advantages of instruction-level parallelism (ILP) \cite[Chapter~10.2.4]{dragon_book}. Whether to do register allocation first, or scheduling first, or to address these problems at the same time is often referred to as, phase-ordering problem. 

In general, you can solve scheduling exactly using algorithms, e.g. integer programming or constraint programming, or one can solve the problem, but without guaranteeing that an optimal solution is found using heuristics. With LLVM there are two main schedulers, i.e. list scheduler and Machine Instruction (MI) scheduler that use heuristics to find a solution, although it is possible to implement a custom scheduler for an architecture at hand. 